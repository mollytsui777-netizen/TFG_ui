# 项目配置文档

本文档提供完整的项目配置和运行指南，帮助助教快速复现项目。文档详细说明如何运行3.1（模型训练、推理和实时对话功能）和3.2（模型效果评价）中的所有功能。

---

## 目录

1. [一、环境准备](#一环境准备)
2. [二、下载 OpenFace（重要）](#二下载-openface重要)
3. [三、克隆仓库](#三克隆仓库)
4. [四、Docker环境配置](#四docker环境配置)
5. [五、项目启动](#五项目启动)
6. [六、功能使用流程（3.1要求）](#六功能使用流程31要求)
7. [七、评测功能（3.2要求）](#七评测功能32要求)
8. [八、验证步骤](#八验证步骤)
9. [九、常见问题](#九常见问题)

---

## 一、环境准备

### 1.1 硬件要求

- **GPU**：NVIDIA GPU，支持CUDA 11.8+，至少8GB显存（推荐16GB+）
- **内存**：至少16GB RAM（推荐32GB+）
- **磁盘空间**：至少100GB可用空间（推荐200GB+）
- **网络**：需要稳定的网络连接（用于下载模型和依赖）

### 1.2 软件要求

- **操作系统**：Ubuntu 20.04+ / CentOS 7+ / 其他Linux发行版
- **Docker**：20.10+（必须）
- **Docker Compose**：1.29+（推荐）
- **NVIDIA Docker**：支持GPU（`nvidia-docker2`）

### 1.3 检查GPU和Docker

**步骤1：检查GPU**

```bash
nvidia-smi
```

**预期输出**：应显示GPU信息，包括GPU型号、显存大小、CUDA版本等。

**如果失败**：检查NVIDIA驱动是否正确安装。

**步骤2：检查Docker**

```bash
docker --version
docker-compose --version
```

**预期输出**：应显示Docker和Docker Compose的版本号。

**如果失败**：需要安装Docker和Docker Compose，参考 `DOCKER完整指南.md`。

**步骤3：检查NVIDIA Docker支持**

```bash
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
```

**预期输出**：应显示与步骤1相同的GPU信息。

**如果失败**：需要安装nvidia-docker2，参考 `DOCKER完整指南.md`。

---

## 二、下载 OpenFace（重要）

### 2.1 为什么需要 OpenFace

**OpenFace** 是一个开源的面部分析工具，用于提取面部动作单元（Action Units, AU）数据。在训练 TalkingGaussian 模型时，**必须**提供 `au.csv` 文件，该文件包含视频中每一帧的面部动作单元数据。

**重要提示**：

- TalkingGaussian 训练需要 AU 数据来控制面部表情
- AU 数据用于生成更自然的面部动画
- **必须在训练前准备好 `au.csv` 文件**
- 如果没有 `au.csv`，训练过程可能会失败或生成效果不佳

### 2.2 下载 OpenFace

#### Windows 系统

**步骤1：访问 OpenFace 官网**

访问 OpenFace GitHub 发布页面：

- 官方仓库：https://github.com/TadasBaltrusaitis/OpenFace
- 发布页面：https://github.com/TadasBaltrusaitis/OpenFace/releases

**步骤2：下载预编译版本**

1. 找到最新版本的 Windows 预编译包
2. 下载：`OpenFace_2.2.0_win_x64.zip`（或更新的版本）
3. 将下载的 ZIP 文件保存到项目根目录或任意位置

**步骤3：解压 OpenFace**

```bash
# 在项目根目录解压（推荐）
unzip OpenFace_2.2.0_win_x64.zip

# 或使用 Windows 资源管理器解压到项目根目录
# 解压后会得到 OpenFace_2.2.0_win_x64/ 目录
```

**验证安装**：

```bash
# 检查 OpenFace 目录是否存在
ls -la OpenFace_2.2.0_win_x64/
# 应看到 FeatureExtraction.exe 文件
```

#### Linux 系统

**步骤1：下载 Linux 版本**

```bash
# 在项目根目录下载
cd /path/to/TFG_ui
wget https://github.com/TadasBaltrusaitis/OpenFace/releases/download/v2.2.0/OpenFace_2.2.0_linux_x64.tar.gz
```

**步骤2：解压**

```bash
tar -xzf OpenFace_2.2.0_linux_x64.tar.gz
cd OpenFace_2.2.0_linux_x64
```

**验证安装**：

```bash
# 检查 FeatureExtraction 可执行文件
ls -la FeatureExtraction
# 应看到可执行文件
```

#### macOS 系统

1. 下载 macOS 版本或从源码编译
2. 参考官方文档进行安装：https://github.com/TadasBaltrusaitis/OpenFace/wiki

### 2.3 使用 OpenFace 预处理视频生成 AU 文件

**重要**：在开始训练之前，必须先使用 OpenFace 处理训练视频，生成 `au.csv` 文件。

#### Windows 系统

```cmd
# 进入 OpenFace 目录
cd OpenFace_2.2.0_win_x64

# 运行 FeatureExtraction 处理训练视频
FeatureExtraction.exe -f "路径\到\训练视频.mp4" -out_dir "输出目录"

# 示例：处理 static/uploads/videos/May.mp4
FeatureExtraction.exe -f "..\..\static\uploads\videos\May.mp4" -out_dir "processed"
```

#### Linux/macOS 系统

```bash
# 进入 OpenFace 目录
cd OpenFace_2.2.0_linux_x64

# 运行 FeatureExtraction 处理训练视频
./FeatureExtraction -f /path/to/training_video.mp4 -out_dir /path/to/output

# 示例：处理 static/uploads/videos/May.mp4
./FeatureExtraction -f ../../static/uploads/videos/May.mp4 -out_dir processed
```

**参数说明**：

- `-f`：输入视频文件路径（训练视频）
- `-out_dir`：输出目录（OpenFace 会在此目录生成 CSV 文件）

**输出文件**：

- OpenFace 会生成一个 CSV 文件，文件名格式为：`<视频名>.csv`
- 例如：处理 `May.mp4` 会生成 `May.csv`
- 该文件包含每一帧的面部特征数据，包括 AU 数据

**处理时间**：

- 根据视频长度，通常需要几分钟到十几分钟
- 处理过程中会显示进度信息

### 2.4 准备 au.csv 文件

**步骤1：检查生成的 CSV 文件**

打开 OpenFace 生成的 CSV 文件（在 `out_dir` 目录中），检查是否包含以下 AU 列：

- ` AU01_r`, ` AU02_r`, ` AU04_r`, ` AU05_r`, ` AU06_r`, ` AU07_r`
- ` AU09_r`, ` AU10_r`, ` AU12_r`, ` AU14_r`, ` AU15_r`, ` AU17_r`
- ` AU20_r`, ` AU23_r`, ` AU25_r`, ` AU26_r`, ` AU45_r`

**步骤2：重命名并移动到项目目录**

```bash
# 将生成的 CSV 文件重命名为 au.csv
# Windows 示例
copy OpenFace_2.2.0_win_x64\processed\May.csv TalkingGaussian\data\May\au.csv

# Linux 示例
cp OpenFace_2.2.0_linux_x64/processed/May.csv TalkingGaussian/data/May/au.csv
```

**重要**：

- `au.csv` 文件必须放在 `TalkingGaussian/data/<项目名>/au.csv`
- 例如：`TalkingGaussian/data/May/au.csv`
- 如果目录不存在，需要先创建：`mkdir -p TalkingGaussian/data/May`

**步骤3：验证 au.csv 文件**

```bash
# 检查文件是否存在
ls -lh TalkingGaussian/data/May/au.csv

# 查看文件前几行（验证格式）
head -5 TalkingGaussian/data/May/au.csv
```

**预期输出**：应看到包含 AU 列的 CSV 文件，每行对应视频的一帧。

### 2.5 注意事项

1. **OpenFace 文件已添加到 .gitignore**：下载的 OpenFace ZIP 文件和解压目录不会被提交到 Git
2. **每个训练视频都需要处理**：如果有多个训练视频，需要分别处理
3. **处理时间**：视频越长，处理时间越长，请耐心等待
4. **如果处理失败**：检查视频文件是否损坏，或尝试使用其他视频

---

## 三、克隆仓库

### 2.1 克隆主仓库

```bash
# 克隆项目仓库（请替换为实际仓库地址）
git clone https://github.com/RongYuFL/TFG_ui.git TFG_ui
cd TFG_ui
```

**验证**：

```bash
ls -la
# 应看到以下目录：backend, static, TalkingGaussian, docker-compose.yml, Dockerfile 等
```

### 2.2 初始化Git子模块（重要！）

**步骤1：进入TalkingGaussian目录**

```bash
cd TalkingGaussian
```

**步骤2：初始化并更新子模块**

```bash
git submodule update --init --recursive
```

**或者使用以下命令（一步完成）**：

```bash
git submodule update --init --recursive --depth 1
```

**子模块说明**：

- `submodules/simple-knn`：用于Gaussian Splatting的KNN加速
- `submodules/diff-gaussian-rasterization`：Gaussian Splatting的光栅化实现

**步骤3：验证子模块**

```bash
# 检查子模块是否已初始化
ls -la submodules/simple-knn/
ls -la submodules/diff-gaussian-rasterization/
```

**预期输出**：两个目录都应包含文件（不是空目录）。

**如果目录为空或不存在**：说明子模块未初始化，需要重新执行步骤2。

### 2.3 如果子模块初始化失败

**方法1：使用代理或镜像（如果有）**

```bash
cd TalkingGaussian
git config --global url."https://github.com/".insteadOf "git@github.com:"
git submodule update --init --recursive
```

**方法2：手动克隆子模块**

```bash
cd TalkingGaussian

# 创建submodules目录（如果不存在）
mkdir -p submodules
cd submodules

# 手动克隆子模块
git clone https://github.com/ashawkey/diff-gaussian-rasterization.git
git clone https://gitlab.inria.fr/bkerbl/simple-knn.git

cd ../..
```

**验证**：再次执行步骤3的验证命令。

---

## 四、Docker环境配置

### 3.1 配置Docker镜像加速（推荐）

如果在中国大陆，建议配置Docker镜像加速：

```bash
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": [
    "https://docker.mirrors.ustc.edu.cn",
    "https://hub-mirror.c.163.com",
    "https://mirror.baidubce.com"
  ],
  "runtimes": {
    "nvidia": {
      "path": "nvidia-container-runtime",
      "runtimeArgs": []
    }
  }
}
EOF

sudo systemctl restart docker
```

**验证**：

```bash
docker info | grep -A 10 "Registry Mirrors"
# 应显示配置的镜像地址
```

### 3.2 使用预构建镜像（推荐，快速开始）

提供了预构建的 Docker 镜像压缩包 `tfg_ui_image.tar.gz`，可以直接导入使用，**无需重新构建**（节省1-2小时）。

#### 方法1：导入预构建镜像（推荐）

**步骤1：确认镜像压缩包位置**

```bash
# 检查镜像压缩包是否存在
ls -lh tfg_ui_image.tar.gz
# 应看到文件大小约 15-20 GB
```

**步骤2：导入镜像**

```bash
# 在项目根目录执行
docker load -i tfg_ui_image.tar.gz
```

**导入时间**：约5-10分钟（取决于磁盘速度）

**预期输出**：
```
Loaded image: tfg_ui:latest
```

**步骤3：验证镜像导入**

```bash
# 查看镜像
docker images | grep tfg_ui
```

**预期输出**：应看到 `tfg_ui:latest` 镜像，大小约 15-20 GB。

```bash
# 测试运行（可选）
docker run --rm --gpus all tfg_ui:latest nvidia-smi
```

**预期输出**：应显示GPU信息。

**注意事项**：

- 镜像压缩包文件较大（约15-20GB），确保有足够的磁盘空间
- 导入后，镜像会占用约15-20GB的Docker存储空间
- 如果导入失败，检查磁盘空间：`df -h` 和 `docker system df`
- 导入成功后，可以删除压缩包以节省空间（可选）

#### 方法2：从压缩包直接运行（临时使用）

如果只是临时测试，也可以直接从压缩包运行（不导入到Docker）：

```bash
# 直接加载并运行（不保存到本地镜像库）
docker load -i tfg_ui_image.tar.gz && docker-compose up -d
```

**注意**：这种方式每次都需要重新加载，不推荐用于生产环境。

### 3.3 构建Docker镜像（备选方案）

如果**没有**预构建镜像压缩包，或者需要自定义构建，可以手动构建镜像：

**步骤1：返回项目根目录**

```bash
cd /path/to/TFG_ui
# 或如果已经在项目根目录
pwd  # 确认当前目录
```

**步骤2：构建主服务镜像**

```bash
# 使用Docker Compose构建（推荐）
docker-compose build tfg_ui

# 或者单独构建
docker build -t tfg_ui:latest -f Dockerfile .
```

**构建时间**：首次构建约需1-2小时（取决于网络速度）

**构建过程**：

1. 下载CUDA基础镜像（约5GB）
2. 安装系统依赖
3. 安装Miniconda和创建Conda环境
4. 安装Python依赖
5. 编译CUDA扩展（diff-gaussian-rasterization、simple-knn）

**查看构建日志**：

```bash
# 实时查看构建日志
docker-compose build --progress=plain tfg_ui

# 或者后台构建并查看日志
docker-compose build tfg_ui 2>&1 | tee build.log
```

**如果构建失败**：

- 检查网络连接
- 检查磁盘空间：`df -h`
- 查看详细错误信息：`docker-compose build --progress=plain tfg_ui`

**构建完成后导出镜像（可选）**：

如果构建成功，可以导出镜像供他人使用：

```bash
# 导出镜像为压缩包
docker save tfg_ui:latest | gzip > tfg_ui_image.tar.gz

# 导出时间：约10-20分钟
# 文件大小：约15-20 GB
```

### 3.4 验证镜像

```bash
# 查看镜像
docker images | grep tfg_ui
```

**预期输出**：应看到 `tfg_ui` 镜像，大小约10-20GB。

```bash
# 测试运行（可选）
docker run --rm --gpus all tfg_ui:latest nvidia-smi
```

**预期输出**：应显示GPU信息。

**镜像选择建议**：

- **有预构建镜像**：使用 `3.2 使用预构建镜像`（快速，推荐）
- **无预构建镜像**：使用 `3.3 构建Docker镜像`（需要1-2小时）
- **需要自定义**：使用 `3.3 构建Docker镜像` 并修改 Dockerfile

---

## 五、项目启动

### 4.1 配置API密钥（可选，仅实时对话功能需要）

如果需要使用实时对话功能，需要配置LLM API：

**步骤1：创建配置文件**

```bash
# 在项目根目录下
mkdir -p backend/config

# 如果存在示例文件，复制它
if [ -f backend/config/api_config.example.json ]; then
    cp backend/config/api_config.example.json backend/config/api_config.json
else
    # 否则创建新文件
    cat > backend/config/api_config.json <<'EOF'
{
  "openai": {
    "api_key": "",
    "base_url": "https://api.openai.com/v1",
    "model": "gpt-3.5-turbo",
    "enabled": false
  },
  "zhipu": {
    "api_key": "",
    "base_url": "https://open.bigmodel.cn/api/paas/v4/",
    "model": "glm-4",
    "enabled": false
  },
  "deepseek": {
    "api_key": "",
    "base_url": "https://api.deepseek.com",
    "model": "deepseek-chat",
    "enabled": false
  }
}
EOF
fi
```

**步骤2：编辑配置文件**

```bash
nano backend/config/api_config.json
# 或使用其他编辑器：vim, gedit 等
```

**配置文件格式**：

```json
{
  "openai": {
    "api_key": "sk-xxxxxxxx",
    "base_url": "https://api.openai.com/v1",
    "model": "gpt-3.5-turbo",
    "enabled": true
  },
  "zhipu": {
    "api_key": "your-zhipu-api-key",
    "base_url": "https://open.bigmodel.cn/api/paas/v4/",
    "model": "glm-4",
    "enabled": true
  },
  "deepseek": {
    "api_key": "sk-your-deepseek-api-key",
    "base_url": "https://api.deepseek.com",
    "model": "deepseek-chat",
    "enabled": true
  }
}
```

**或者使用环境变量**（优先级更高）：

```bash
export OPENAI_API_KEY="sk-xxxxxxxx"
export ZHIPU_API_KEY="your-zhipu-api-key"
export DEEPSEEK_API_KEY="sk-your-deepseek-api-key"
```

**注意**：如果不使用实时对话功能，可以跳过此步骤。

### 4.2 启动服务

#### 方式1：使用Docker Compose（推荐）

**步骤1：启动服务**

```bash
# 在项目根目录下
docker-compose up -d
```

**步骤2：查看日志**

```bash
docker-compose logs -f tfg_ui
```

**预期输出**：应看到服务启动日志，包括：

- Flask应用启动信息
- 端口监听信息（如：`Running on http://0.0.0.0:5001`）

**步骤3：检查服务状态**

```bash
docker-compose ps
```

**预期输出**：`tfg_ui` 服务状态应为 `Up`。

**停止服务**：

```bash
docker-compose down
```

#### 方式2：直接使用Docker

```bash
# 启动容器
docker run -d \
  --name tfg_ui \
  --gpus all \
  -p 5001:5001 \
  -v $(pwd)/static:/app/static \
  -v $(pwd)/TalkingGaussian/data:/app/TalkingGaussian/data \
  -v $(pwd)/TalkingGaussian/output:/app/TalkingGaussian/output \
  -v $(pwd)/backend/config:/app/backend/config \
  -e CUDA_VISIBLE_DEVICES=0 \
  tfg_ui:latest

# 查看日志
docker logs -f tfg_ui
```

**停止容器**：

```bash
docker stop tfg_ui
docker rm tfg_ui
```

### 4.3 访问服务

**步骤1：检查服务是否运行**

```bash
# 方式1：使用Docker Compose
docker-compose ps

# 方式2：使用Docker
docker ps | grep tfg_ui
```

**步骤2：访问Web界面**

在浏览器中访问：

```
http://服务器IP:5001
```

或本地访问：

```
http://localhost:5001
```

**预期结果**：应看到项目首页，包含以下功能入口：

- 模型训练
- 视频生成
- 实时对话

**如果无法访问**：

1. 检查防火墙：`sudo ufw allow 5001` 或 `sudo firewall-cmd --add-port=5001/tcp --permanent`
2. 检查服务日志：`docker-compose logs tfg_ui`
3. 检查端口占用：`netstat -tuln | grep 5001`

---

## 六、功能使用流程（3.1要求）

本节详细说明如何运行3.1要求的所有功能：模型训练、视频生成和实时对话。

### 5.1 模型训练页面

**功能**：训练个性化的3D说话人头像模型

**完整使用流程**：

#### 步骤1：准备训练数据

**1.1 准备训练视频**

```bash
# 在服务器上创建上传目录（如果不存在）
mkdir -p static/uploads/videos

# 上传训练视频到 static/uploads/videos/ 目录
# 例如：static/uploads/videos/May.mp4
```

**视频要求**：

- 清晰的人脸视频
- 建议时长1-5分钟
- 分辨率至少720p
- 格式：MP4（推荐）

**1.2 准备AU文件**

按照第2节说明，使用OpenFace生成 `au.csv` 文件。

**如果是在本地生成，需要上传到服务器**：

**方式1：通过前端上传接口（推荐）**

```bash
# 使用 curl 命令上传
curl -F "project_id=May" \
     -F "au_file=@au.csv" \
     http://服务器IP:5001/upload_au
```

**方式2：直接上传到服务器**

```bash
# 使用 SCP
scp au.csv user@server:/path/to/TFG_ui/TalkingGaussian/data/May/au.csv

# 或使用 rsync
rsync -avz au.csv user@server:/path/to/TFG_ui/TalkingGaussian/data/May/au.csv
```

**文件位置**：

- `au.csv` 文件必须放在 `TalkingGaussian/data/<项目ID>/au.csv`
- 例如：`TalkingGaussian/data/May/au.csv`
- 如果目录不存在，需要先创建：`mkdir -p TalkingGaussian/data/May`

**验证数据准备**：

```bash
# 检查训练视频
ls -lh static/uploads/videos/May.mp4

# 检查AU文件
ls -lh TalkingGaussian/data/May/au.csv
```

#### 步骤2：进入训练页面

1. 在浏览器中访问：`http://服务器IP:5001`
2. 点击"模型训练"按钮
3. 或直接访问：`http://服务器IP:5001/model_training`

**预期结果**：应看到模型训练页面，包含参数配置表单。

#### 步骤3：配置训练参数

在训练页面中配置以下参数：

- **模型选择**：选择 "TalkingGaussian"
- **训练视频**：选择或输入视频路径（如 `static/uploads/videos/May.mp4`）
- **GPU选择**：选择使用的GPU（如 "GPU0"）
- **训练轮数**：设置训练轮数（默认值即可，通常不需要修改）

**注意**：确保训练视频路径和AU文件路径正确。

#### 步骤4：开始训练

1. 点击"开始训练"按钮
2. 页面会显示训练进度和日志

**系统自动执行步骤**：

1. 数据预处理（提取帧、面部跟踪、3DMM拟合）
2. 三阶段训练：
   - 嘴部训练
   - 面部训练
   - 融合训练

**训练时间**：根据视频长度和GPU性能，通常需要30分钟到数小时

**查看训练日志**（在服务器上）：

```bash
# 进入Docker容器
docker exec -it tfg_ui bash

# 查看训练输出
tail -f /app/TalkingGaussian/output/May/train.log
# 或查看Docker日志
docker-compose logs -f tfg_ui
```

#### 步骤5：查看训练结果

训练完成后，页面会显示：

- 模型路径（如 `output/May`）
- 预览视频（训练过程中的预览）
- 参考音频（从训练视频提取的音频）

**验证训练结果**：

```bash
# 检查模型文件
ls -lh TalkingGaussian/output/May/
# 应看到以下文件：
# - chkpnt_face_latest.pth（面部模型）
# - chkpnt_mouth_latest.pth（嘴部模型）
# - chkpnt_fuse_latest.pth（融合模型）

# 检查参考音频
ls -lh static/uploads/audios/May_reference.wav
```

**训练输出位置**：

- 模型文件：`TalkingGaussian/output/<项目名>/`
- 参考音频：`static/uploads/audios/<项目名>_reference.wav`

**注意事项**：

- 训练过程中不要关闭浏览器或停止服务
- 训练会占用大量GPU显存，建议单独使用一个GPU
- 训练数据会自动保存在 `TalkingGaussian/data/<项目名>/` 目录

### 5.2 视频生成页面

**功能**：根据音频生成对应的说话人头像视频

**完整使用流程**：

#### 步骤1：进入视频生成页面

1. 在浏览器中访问：`http://服务器IP:5001`
2. 点击"视频生成"按钮
3. 或直接访问：`http://服务器IP:5001/video_generation`

**预期结果**：应看到视频生成页面，包含参数配置表单。

#### 步骤2：配置生成参数

在视频生成页面中配置以下参数：

- **模型选择**：选择 "TalkingGaussian"
- **模型路径**：输入训练好的模型路径（如 `output/May`）
- **参考音频**：选择或输入音频文件路径
  - 可以使用训练时生成的参考音频：`static/uploads/audios/May_reference.wav`
  - 或使用其他音频文件（WAV、MP3、M4A等格式）
- **GPU选择**：选择使用的GPU
- **渲染细节等级**：选择0-3（数字越大，细节越多，但速度越慢，默认2）

**验证模型路径**：

```bash
# 确保模型路径存在
ls -lh TalkingGaussian/output/May/
```

#### 步骤3：开始生成

1. 点击"生成视频"按钮
2. 页面会显示生成进度

**生成时间**：根据音频长度和GPU性能，通常需要1-5分钟

**查看生成日志**：

```bash
docker-compose logs -f tfg_ui
```

#### 步骤4：查看生成结果

生成完成后，页面会显示生成的视频。

**验证生成结果**：

```bash
# 检查生成的视频
ls -lh static/videos/
# 应看到带时间戳的视频文件，如：chat_response_20231224_123456.mp4
```

**生成视频位置**：

- 视频会自动保存到 `static/videos/` 目录（带时间戳，不会覆盖）

**注意事项**：

- 确保模型路径正确（训练完成后会显示）
- 音频格式支持：WAV、MP3、M4A等（会自动转换）
- 生成的视频会包含音频轨道

### 5.3 实时对话页面

**功能**：完整的语音交互流程（语音识别 → 大语言模型 → 语音合成 → 视频生成）

**完整使用流程**：

#### 步骤1：进入对话页面

1. 在浏览器中访问：`http://服务器IP:5001`
2. 点击"实时对话"按钮
3. 或直接访问：`http://服务器IP:5001/chat_system`

**预期结果**：应看到实时对话页面，包含录音、参数配置和结果显示区域。

#### 步骤2：配置对话参数

在对话页面中配置以下参数：

- **模型路径**：输入训练好的模型路径（如 `output/May`）
- **GPU选择**：选择使用的GPU
- **LLM API选择**：选择使用的大语言模型API（OpenAI、Zhipu、DeepSeek）
  - **注意**：需要先配置API密钥（见4.1节）
- **语言类型**：选择中文或英文
- **语速**：调节语音合成速度（0.5-2.0倍速，默认1.0）
- **语音克隆参考音频**：选择三种方式之一
  - **当前录音**：使用刚录制的音频作为参考
  - **预设音色**：使用系统预设的音频（default、cross_lingual等）
  - **自定义音频**：上传自定义的参考音频文件

**验证API配置**：

```bash
# 检查API配置文件
cat backend/config/api_config.json
# 确保至少有一个API的enabled为true，且api_key已配置
```

#### 步骤3：开始对话

**3.1 录音**

1. 点击"开始录音"按钮
2. 说话（建议在安静环境中，清晰发音）
3. 点击"停止录音"按钮

**3.2 生成视频**

1. 点击"生成视频"按钮
2. 页面会显示处理进度

**系统自动执行步骤**：

1. **ASR语音识别**：将录音转换为文本（`input.wav` → `input.txt`）
2. **LLM生成回复**：调用大语言模型生成回复（`input.txt` → `output.txt`）
3. **TTS语音合成**：将回复文本合成为语音（`output.txt` + 参考音频 → `tts_output.wav`）
4. **视频生成**：根据合成语音生成说话人头像视频（`tts_output.wav` → `chat_response.mp4`）

**处理时间**：根据音频长度和GPU性能，通常需要1-3分钟

**查看处理日志**：

```bash
docker-compose logs -f tfg_ui
```

#### 步骤4：查看对话结果

生成完成后，页面会显示：

- 识别的文本（ASR结果）
- LLM生成的回复文本
- 生成的说话人头像视频

**验证对话结果**：

```bash
# 检查生成的视频
ls -lh static/videos/
# 应看到 chat_response_*.mp4 文件

# 检查中间文件（可选）
ls -lh static/uploads/audios/
# 可能看到 input.wav, tts_output.wav 等文件
```

**注意事项**：

- 需要配置LLM API密钥（见4.1节）
- 录音时建议在安静环境中，清晰发音
- 对话过程可能需要1-3分钟（取决于音频长度和GPU性能）
- 如果API调用失败，检查网络连接和API密钥配置

---

## 七、评测功能（3.2要求）

本节详细说明如何运行3.2要求的所有评测指标：NIQE、PSNR、FID、SSIM、LSE-C、LSE-D。

### 6.1 评测概述

项目支持多种评测指标，用于评估生成视频的质量：

- **PSNR**（Peak Signal-to-Noise Ratio）：峰值信噪比，衡量图像质量
- **SSIM**（Structural Similarity Index）：结构相似性指数
- **NIQE**（Natural Image Quality Evaluator）：自然图像质量评估（无需参考图像）
- **FID**（Fréchet Inception Distance）：Fréchet Inception距离
- **LSE-C**（Lip Sync Error - Confidence）：唇形同步错误（置信度）
- **LSE-D**（Lip Sync Error - Distance）：唇形同步错误（距离）

### 6.2 评测前提条件

**必须完成模型训练**：

- 至少完成一次模型训练（见6.1节）
- 训练完成后，会有生成的视频可用于评测

**评测数据准备**：

- **预测视频**：训练完成后生成的视频（位于 `TalkingGaussian/output/<项目名>/train/` 或 `test/` 目录）
- **真实视频**：原始训练视频（位于 `TalkingGaussian/data/<项目名>/<项目名>.mp4`）

**重要提示**：

- 评测使用的是训练完成后生成的视频，不是实时对话生成的视频
- 预测视频通常在 `output/<项目名>/train/` 目录下（如果使用了训练参数）
- 如果结果在 `test/` 目录，需要确认路径

**验证数据准备**：

```bash
# 检查预测视频
ls -lh TalkingGaussian/output/May/train/
# 或
ls -lh TalkingGaussian/output/May/test/

# 检查真实视频
ls -lh TalkingGaussian/data/May/May.mp4
```

### 6.3 评测方法

#### 方法1：使用统一评测脚本（推荐）

**步骤1：进入Docker容器**

```bash
docker exec -it tfg_ui bash
```

**步骤2：进入评测目录**

```bash
cd TalkingGaussian/evaluation
```

**步骤3：运行统一评测脚本**

使用 `run_eval.sh` 脚本进行评测（基于官方的 `eval_all.py`）：

```bash
bash run_eval.sh \
  --pred-video /app/TalkingGaussian/output/May/train/result.mp4 \
  --gt-video /app/TalkingGaussian/data/May/May.mp4 \
  --all \
  --json-out /app/test_metrics_may.json
```

**参数说明**：

- `--pred-video`：预测视频文件路径（单个视频文件）
- `--gt-video`：真实视频文件路径（单个视频文件）
- `--all`：运行所有可用的评测指标（PSNR/SSIM、NIQE、FID、LSE）
- `--json-out`：输出JSON文件路径（可选，默认输出到控制台）

**评测系统生成的视频**：

如果视频是通过Web界面生成的，通常保存在 `static/videos/` 目录：

```bash
# 查找系统生成的视频文件
ls -lh /app/static/videos/
# TalkingGaussian生成的视频：talkinggaussian_*.mp4
# SyncTalk生成的视频：<模型名>_*.mp4

# 使用找到的视频进行评测
bash run_eval.sh \
  --pred-video /app/static/videos/talkinggaussian_xxx.mp4 \
  --gt-video /app/TalkingGaussian/data/May/May.mp4 \
  --all \
  --json-out /app/test_metrics_may.json
```

**如果预测视频在test目录**：

```bash
bash run_eval.sh \
  --pred-video /app/TalkingGaussian/output/May/test/result.mp4 \
  --gt-video /app/TalkingGaussian/data/May/May.mp4 \
  --all \
  --json-out /app/test_metrics_may.json
```

**选择性运行指标**：

如果只想运行部分指标，可以分别指定：

```bash
# 只运行帧级指标（PSNR/SSIM）
bash run_eval.sh \
  --pred-video /app/TalkingGaussian/output/May/train/result.mp4 \
  --gt-video /app/TalkingGaussian/data/May/May.mp4 \
  --frame \
  --json-out /app/test_metrics_may.json

# 运行帧级指标 + NIQE + FID
bash run_eval.sh \
  --pred-video /app/TalkingGaussian/output/May/train/result.mp4 \
  --gt-video /app/TalkingGaussian/data/May/May.mp4 \
  --frame --niqe --fid \
  --json-out /app/test_metrics_may.json
```

**步骤4：查看评测结果**

评测完成后，结果会保存到指定的JSON文件：

```bash
# 在容器内查看
cat /app/test_metrics_may.json

# 或退出容器后查看
exit
cat test_metrics_may.json
```

**输出结果格式**：

```json
{
  "PSNR": 28.5,
  "SSIM": 0.85,
  "NIQE": 5.2,
  "FID": 12.3,
  "LSE-C": 6.8,
  "LSE-D": 2.1
}
```

**评测时间**：

- 完整评测可能需要30分钟到1小时（取决于视频长度和GPU性能）
- 各指标耗时：
  - PSNR/SSIM：较快（几秒到几分钟）
  - NIQE：中等（几分钟）
  - FID：较慢（可能需要10-30分钟）
  - LSE：较慢（可能需要10-30分钟）

#### 方法2：单独运行各指标

如果统一脚本失败，可以单独运行各指标：

**步骤1：进入Docker容器**

```bash
docker exec -it tfg_ui bash
cd TalkingGaussian/evaluation
```

**步骤2：运行PSNR/SSIM（帧级指标）**

```bash
conda run -n talking_gaussian --no-capture-output \
  python eval_frame_metrics.py \
  /app/TalkingGaussian/output/May/train/result.mp4 \
  /app/TalkingGaussian/data/May/May.mp4
```

**预期输出**：应显示PSNR和SSIM数值。

**步骤3：运行NIQE（需要先提取帧）**

```bash
# 提取预测视频的帧
conda run -n talking_gaussian --no-capture-output \
  python extract_frames.py \
  /app/TalkingGaussian/output/May/train/result.mp4 \
  /app/TalkingGaussian/evaluation/_work/pred_frames

# 运行NIQE评测
conda run -n tg_niqe --no-capture-output \
  python eval_niqe.py \
  /app/TalkingGaussian/evaluation/_work/pred_frames
```

**预期输出**：应显示NIQE数值。

**步骤4：运行FID（需要预测和真实帧）**

```bash
# 提取真实视频的帧
conda run -n talking_gaussian --no-capture-output \
  python extract_frames.py \
  /app/TalkingGaussian/data/May/May.mp4 \
  /app/TalkingGaussian/evaluation/_work/gt_frames

# 运行FID评测（需要先完成步骤3的帧提取）
conda run -n tg_eval --no-capture-output \
  python eval_fid.py \
  /app/TalkingGaussian/evaluation/_work/pred_frames \
  /app/TalkingGaussian/evaluation/_work/gt_frames
```

**预期输出**：应显示FID数值。

**步骤5：运行LSE-C/LSE-D（需要设置SyncNet）**

```bash
# 首次运行需要设置SyncNet（只需运行一次）
bash setup_syncnet.sh
bash setup_lse.sh

# 运行LSE评测
conda run -n talking_gaussian --no-capture-output \
  python eval_lse.py \
  /app/TalkingGaussian/output/May/train \
  --preset wav2lip-real
```

**预期输出**：应显示LSE-C和LSE-D数值。

**注意**：首次运行LSE评测会自动下载SyncNet和Wav2Lip相关代码，可能需要较长时间。

### 6.4 评测注意事项

1. **数据路径**：

   - 在Docker容器内，路径以 `/app/` 开头
   - 预测视频通常在 `output/<项目名>/train/` 目录
   - 如果结果在 `test/` 目录，需要确认路径
2. **LSE指标**：

   - 需要先运行 `setup_syncnet.sh` 和 `setup_lse.sh` 设置环境
   - 首次运行会自动下载SyncNet和Wav2Lip相关代码
   - 如果下载失败，检查网络连接
3. **评测环境**：

   - 不同指标使用不同的Conda环境：
     - `talking_gaussian`：PSNR/SSIM/LSE
     - `tg_niqe`：NIQE
     - `tg_eval`：FID
   - 如果环境不存在，检查Docker构建是否成功
4. **评测时间**：

   - PSNR/SSIM：较快（几秒到几分钟）
   - NIQE：中等（几分钟）
   - FID：较慢（可能需要10-30分钟）
   - LSE：较慢（可能需要10-30分钟）
5. **磁盘空间**：

   - 评测过程会生成临时文件（提取的帧等）
   - 确保有足够的磁盘空间（至少10GB）

### 6.5 评测结果解读

- **PSNR**：数值越大越好（通常>25为较好）
- **SSIM**：数值越大越好（范围0-1，通常>0.8为较好）
- **NIQE**：数值越小越好（通常<6为较好）
- **FID**：数值越小越好（通常<20为较好）
- **LSE-C**：数值越小越好（通常<10为较好）
- **LSE-D**：数值越小越好（通常<3为较好）

### 6.6 完整评测流程示例

**示例：评测 "May" 项目的模型**

```bash
# 1. 进入Docker容器
docker exec -it tfg_ui bash

# 2. 进入评测目录
cd TalkingGaussian/evaluation

# 3. 验证数据文件存在
ls -lh /app/TalkingGaussian/output/May/train/
ls -lh /app/TalkingGaussian/data/May/May.mp4

# 4. 运行统一评测脚本
bash run_eval.sh \
  --pred-video /app/TalkingGaussian/output/May/train/result.mp4 \
  --gt-video /app/TalkingGaussian/data/May/May.mp4 \
  --all \
  --json-out /app/test_metrics_may.json

# 5. 查看结果
cat /app/test_metrics_may.json

# 6. 退出容器
exit

# 7. 在宿主机上查看结果
cat test_metrics_may.json
```

---

## 八、验证步骤

本节提供完整的验证步骤，确保所有功能正常运行。

### 7.1 环境验证

**步骤1：验证GPU和Docker**

```bash
# 检查GPU
nvidia-smi
# 应显示GPU信息

# 检查Docker
docker --version
docker-compose --version
# 应显示版本号

# 检查NVIDIA Docker
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
# 应显示GPU信息
```

**步骤2：验证项目结构**

```bash
cd TFG_ui
ls -la
# 应看到：backend, static, TalkingGaussian, docker-compose.yml, Dockerfile 等

cd TalkingGaussian
ls -la submodules/
# 应看到：simple-knn, diff-gaussian-rasterization 目录（非空）
```

**步骤3：验证Docker镜像**

```bash
docker images | grep tfg_ui
# 应看到 tfg_ui 镜像
```

### 7.2 服务验证

**步骤1：验证服务启动**

```bash
docker-compose up -d
docker-compose ps
# 应显示 tfg_ui 服务状态为 Up
```

**步骤2：验证Web访问**

在浏览器中访问：`http://服务器IP:5001`

**预期结果**：应看到项目首页，包含：

- 模型训练
- 视频生成
- 实时对话

**步骤3：验证API接口**

```bash
# 测试健康检查接口（如果存在）
curl http://服务器IP:5001/health

# 或测试根路径
curl http://服务器IP:5001/
# 应返回HTML页面
```

### 7.3 功能验证

**步骤1：验证模型训练功能**

1. 访问训练页面：`http://服务器IP:5001/model_training`
2. 检查页面是否正常加载
3. 检查参数配置表单是否显示

**步骤2：验证视频生成功能**

1. 访问视频生成页面：`http://服务器IP:5001/video_generation`
2. 检查页面是否正常加载
3. 检查参数配置表单是否显示

**步骤3：验证实时对话功能**

1. 访问对话页面：`http://服务器IP:5001/chat_system`
2. 检查页面是否正常加载
3. 检查录音功能是否可用（需要浏览器权限）

### 7.4 数据验证

**步骤1：验证训练数据**

```bash
# 检查训练视频
ls -lh static/uploads/videos/
# 应看到训练视频文件

# 检查AU文件
ls -lh TalkingGaussian/data/May/au.csv
# 应看到au.csv文件
```

**步骤2：验证训练结果**

```bash
# 检查模型文件
ls -lh TalkingGaussian/output/May/
# 应看到：chkpnt_face_latest.pth, chkpnt_mouth_latest.pth, chkpnt_fuse_latest.pth

# 检查参考音频
ls -lh static/uploads/audios/May_reference.wav
# 应看到参考音频文件
```

**步骤3：验证生成结果**

```bash
# 检查生成的视频
ls -lh static/videos/
# 应看到生成的视频文件
```

### 7.5 评测验证

**步骤1：验证评测数据**

```bash
# 检查预测视频
ls -lh TalkingGaussian/output/May/train/
# 或
ls -lh TalkingGaussian/output/May/test/

# 检查真实视频
ls -lh TalkingGaussian/data/May/May.mp4
```

**步骤2：验证评测脚本**

```bash
docker exec -it tfg_ui bash
cd TalkingGaussian/evaluation
ls -la
# 应看到：run_eval.sh, eval_frame_metrics.py 等脚本
```

**步骤3：运行测试评测**

```bash
# 在容器内
bash run_eval.sh \
  --pred-video /app/TalkingGaussian/output/May/train/result.mp4 \
  --gt-video /app/TalkingGaussian/data/May/May.mp4 \
  --all \
  --json-out /app/test_metrics_may.json

# 检查结果
cat /app/test_metrics_may.json
# 应看到包含所有评测指标的JSON文件
```

---

## 九、常见问题

### 8.1 Git子模块相关问题

**问题**：克隆后子模块目录为空

**解决**：

```bash
cd TalkingGaussian
git submodule update --init --recursive
```

**验证**：

```bash
ls -la submodules/simple-knn/
ls -la submodules/diff-gaussian-rasterization/
# 应看到文件（非空）
```

**问题**：子模块初始化失败（网络问题）

**解决**：

- 使用代理或VPN
- 或手动克隆子模块（见2.3节）

### 8.2 Docker构建问题

**问题**：构建时网络超时

**解决**：

- 配置Docker镜像加速（见3.1节）
- 或使用代理
- 检查网络连接

**问题**：构建时磁盘空间不足

**解决**：

```bash
# 清理Docker缓存
docker system prune -a

# 检查磁盘空间
df -h

# 扩展磁盘空间或清理其他文件
```

**问题**：构建时CUDA编译失败

**解决**：

- 确保使用 `devel` 版本的CUDA镜像（Dockerfile中已配置）
- 检查GPU驱动和CUDA版本兼容性
- 查看详细错误日志：`docker-compose build --progress=plain tfg_ui`

### 8.3 服务启动问题

**问题**：无法访问服务（5001端口）

**解决**：

```bash
# 检查防火墙
sudo ufw allow 5001
# 或
sudo firewall-cmd --add-port=5001/tcp --permanent
sudo firewall-cmd --reload

# 检查服务是否运行
docker-compose ps

# 查看日志
docker-compose logs tfg_ui

# 检查端口占用
netstat -tuln | grep 5001
```

**问题**：服务启动后立即退出

**解决**：

```bash
# 查看详细日志
docker-compose logs tfg_ui

# 检查配置文件
cat backend/config/api_config.json

# 检查数据目录权限
ls -la static/
ls -la TalkingGaussian/data/
```

### 8.4 训练相关问题

**问题**：训练时显存不足

**解决**：

- 使用显存更大的GPU
- 或减少训练视频分辨率
- 检查其他进程是否占用GPU：`nvidia-smi`

**问题**：训练时找不到au.csv

**解决**：

```bash
# 检查文件路径
ls -lh TalkingGaussian/data/May/au.csv

# 检查文件名（必须是小写au.csv）
ls -lh TalkingGaussian/data/May/ | grep -i au

# 检查文件权限
chmod 644 TalkingGaussian/data/May/au.csv
```

**问题**：训练结果在 `test/` 目录而不是 `train/` 目录

**解决**：

- 这是正常的，取决于训练时使用的参数
- 评测时使用 `test/` 目录的视频即可
- 或修改训练参数使其输出到 `train/` 目录

### 8.5 视频生成问题

**问题**：生成视频时找不到模型文件

**解决**：

```bash
# 检查模型路径
ls -lh TalkingGaussian/output/May/

# 确保模型文件存在
ls -lh TalkingGaussian/output/May/chkpnt_*.pth
```

**问题**：生成的视频没有声音

**解决**：

- 检查音频文件是否存在
- 检查音频格式是否支持
- 查看生成日志：`docker-compose logs tfg_ui`

### 8.6 实时对话问题

**问题**：API调用失败

**解决**：

```bash
# 检查API配置
cat backend/config/api_config.json

# 检查API密钥是否正确
# 检查网络连接
curl https://api.openai.com/v1/models

# 查看详细错误日志
docker-compose logs tfg_ui | grep -i error
```

**问题**：录音功能不可用

**解决**：

- 检查浏览器权限（允许麦克风访问）
- 使用Chrome或Firefox浏览器
- 检查HTTPS设置（某些浏览器要求HTTPS才能使用麦克风）

### 8.7 评测相关问题

**问题**：评测脚本找不到视频文件

**解决**：

```bash
# 确认视频路径正确
docker exec -it tfg_ui bash
ls -lh /app/TalkingGaussian/output/May/train/
ls -lh /app/TalkingGaussian/data/May/May.mp4

# 在Docker容器内使用绝对路径（以 `/app/` 开头）
```

**问题**：LSE指标运行失败

**解决**：

```bash
# 先运行设置脚本
docker exec -it tfg_ui bash
cd TalkingGaussian/evaluation
bash setup_syncnet.sh
bash setup_lse.sh

# 检查SyncNet相关文件是否存在
ls -la syncnet/
```

**问题**：评测环境不存在

**解决**：

```bash
# 检查Conda环境
docker exec -it tfg_ui bash
conda env list
# 应看到：talking_gaussian, tg_niqe, tg_eval

# 如果环境不存在，重新构建Docker镜像
docker-compose build tfg_ui
```

**问题**：评测结果异常（数值过大或过小）

**解决**：

- 检查预测视频和真实视频是否对应
- 检查视频格式和分辨率是否一致
- 查看评测日志：`docker-compose logs tfg_ui`

### 8.8 其他问题

**问题**：Docker容器内无法访问GPU

**解决**：

```bash
# 检查NVIDIA Docker运行时
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

# 检查docker-compose.yml中的GPU配置
cat docker-compose.yml | grep -i gpu

# 重启Docker服务
sudo systemctl restart docker
```

**问题**：文件权限问题

**解决**：

```bash
# 检查文件权限
ls -la static/
ls -la TalkingGaussian/data/
ls -la TalkingGaussian/output/

# 修改权限（如果需要）
chmod -R 755 static/
chmod -R 755 TalkingGaussian/data/
chmod -R 755 TalkingGaussian/output/
```

---

## 十、总结

本文档详细说明了如何运行3.1（模型训练、推理和实时对话功能）和3.2（模型效果评价）中的所有功能。按照文档步骤操作，应能成功复现所有功能。

**关键步骤总结**：

1. **环境准备**：确保GPU、Docker和NVIDIA Docker正确安装
2. **项目克隆**：克隆仓库并初始化Git子模块
3. **Docker构建**：构建Docker镜像（可能需要1-2小时）
4. **服务启动**：启动Docker服务并访问Web界面
5. **数据准备**：准备训练视频和AU文件
6. **模型训练**：通过Web界面训练模型
7. **视频生成**：使用训练好的模型生成视频
8. **实时对话**：测试完整的语音交互流程
9. **模型评测**：运行所有评测指标并查看结果

**如果遇到问题**：

1. 查看本文档的"常见问题"部分
2. 查看Docker日志：`docker-compose logs tfg_ui`
3. 检查相关文档：`DOCKER完整指南.md`、`项目结构说明.md`
4. 联系项目维护者

**验收检查清单**：

- [ ] 环境准备完成（GPU、Docker验证通过）
- [ ] 项目克隆和子模块初始化完成
- [ ] Docker镜像构建成功
- [ ] 服务启动成功，Web界面可访问
- [ ] 模型训练功能正常（至少完成一次训练）
- [ ] 视频生成功能正常（至少生成一个视频）
- [ ] 实时对话功能正常（至少完成一次对话）
- [ ] 评测功能正常（所有指标都能运行并输出结果）

---

## 附录

### A. 相关文档

- `项目结构说明.md`：详细的项目结构说明
- `DOCKER完整指南.md`：Docker详细使用文档
- `README.md`：项目基本说明

### B. 技术支持

如遇到问题，请：

1. 查看相关文档
2. 检查日志文件
3. 联系项目维护者
